from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, List, Type, TypeVar
import logging
import uuid
import json
import os
import re

from pydantic import BaseModel, ValidationError
# Remove direct OpenAI import if LiteLLM handles it, or keep if OpenAILLMProvider is kept as a fallback/specific option
# from openai import AsyncOpenAI, APIError 
from dotenv import load_dotenv

from chungoid.utils.prompt_manager import PromptManager, PromptDefinition, PromptRenderError, PromptLoadError

# NEW: Add LiteLLM import
# Ensure litellm is added to requirements.txt
try:
    import litellm
    from litellm import acompletion # For async calls
    # litellm.set_verbose = True # Optional: for debugging LiteLLM
except ImportError:
    logging.getLogger(__name__).error("LiteLLM library is not installed. Please install it with `pip install litellm`.")
    litellm = None # Allow an ImportError to be caught by callers


logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseModel)

class LLMProvider(ABC):
    """
    Abstract Base Class for an LLM interaction provider.
    Defines a standard interface for sending prompts to an LLM and receiving responses.
    """

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None, # Added for consistency
        response_format: Optional[Dict[str, str]] = None, # Added for consistency
        **kwargs: Any  # For provider-specific parameters
    ) -> str:
        """
        Sends a prompt to the LLM and returns the generated text response.

        Args:
            prompt: The main prompt string for the LLM.
            model_id: Optional; specific model identifier if overriding a default.
            temperature: Optional; sampling temperature.
            max_tokens: Optional; maximum number of tokens to generate.
            system_prompt: Optional; system message for the LLM.
            response_format: Optional; dictionary specifying response format (e.g., {"type": "json_object"}).
            kwargs: Additional provider-specific arguments.

        Returns:
            The LLM's generated text response.
        """
        pass


class MockLLMProvider(LLMProvider):
    """
    A mock implementation of LLMProvider for testing and development.
    It can be configured to return predefined responses or echo prompts.
    """

    def __init__(self, predefined_responses: Optional[Dict[str, str]] = None):
        self.predefined_responses = predefined_responses if predefined_responses else {}
        # Track calls for testing purposes
        self.calls: List[Dict[str, Any]] = []

    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
        response_format: Optional[Dict[str, str]] = None,
        **kwargs: Any
    ) -> str:
        self.calls.append({
            "prompt": prompt,
            "model_id": model_id,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "system_prompt": system_prompt,
            "response_format": response_format,
            "kwargs": kwargs
        })

        logger.info(f"MockLLMProvider received prompt (first 100 chars): {prompt[:100]}...")
        logger.info(f"MockLLMProvider args: model_id={model_id}, temp={temperature}, max_tokens={max_tokens}, system_prompt_present={bool(system_prompt)}")

        # Check if a direct match for the prompt exists in predefined_responses
        if prompt in self.predefined_responses:
            response = self.predefined_responses[prompt]
            logger.info(f"MockLLMProvider returning predefined response for direct prompt match: {response}")
            return response

        # Check for partial matches (e.g., if prompt starts with a key)
        for key, predefined_response in self.predefined_responses.items():
            if prompt.strip().startswith(key.strip()):
                logger.info(f"MockLLMProvider returning predefined response for partial match (key: '{key}'): {predefined_response}")
                return predefined_response
        
        # Default behavior: return a stringified minimal valid EnhancedMasterExecutionPlan
        plan_id_mock = f"mock_plan_{str(uuid.uuid4())[:4]}"
        default_plan_dict = {
            "id": plan_id_mock,
            "name": f"Mock Plan for {prompt[:50]}...",
            "description": "A minimal mock plan generated by MockLLMProvider.",
            "version": "2.0.0",
            "initial_stage": "mock_stage_1",
            "start_stage": "mock_stage_1",  # For backward compatibility
            "stages": {
                "mock_stage_1": {
                    "id": "mock_stage_1",
                    "number": 1,
                    "name": "Mock Stage 1",
                    "description": "This is a mock stage.",
                    "task_type": "requirements_analysis",
                    "required_capabilities": ["requirements_gathering", "stakeholder_analysis"],
                    "agent_id": "SystemRequirementsGatheringAgent_v1",  # Use an agent that exists
                    "inputs": {
                        "user_goal": "Create a simple Python hello world script",
                        "project_context": "Simple Python project"
                    },
                    "next_stage": "mock_stage_2",
                    "condition": None,
                    "next_stage_true": None,
                    "next_stage_false": None,
                    "on_error": "mock_intervention_stage"
                },
                "mock_stage_2": {
                    "id": "mock_stage_2", 
                    "number": 2,
                    "name": "Mock Stage 2",
                    "description": "Code generation stage.",
                    "task_type": "code_generation",
                    "required_capabilities": ["systematic_implementation", "code_generation"],
                    "agent_id": "SmartCodeGeneratorAgent_v1",  # Use an agent that exists
                    "inputs": {
                        "task_id": "hello_world_generation",
                        "target_file_path": "hello_world.py",
                        "code_specification": "Create a Python script that prints 'Hello, World!' to the console"
                    },
                    "next_stage": None,
                    "condition": None,
                    "next_stage_true": None,
                    "next_stage_false": None,
                    "on_error": "mock_intervention_stage"
                },
                "mock_intervention_stage": {
                    "id": "mock_intervention_stage",
                    "number": 3,
                    "name": "Mock Intervention Stage",
                    "description": "Human intervention stage for error handling.",
                    "task_type": "human_intervention",
                    "required_capabilities": ["human_interaction", "system_intervention"],
                    "agent_id": "SystemInterventionAgent_v1",  # Use the agent we created
                    "inputs": {
                        "prompt_message_for_user": "An error occurred during execution. Please review and provide guidance.",
                        "intervention_type": "error_handling"
                    },
                    "next_stage": None,
                    "condition": None,
                    "next_stage_true": None,
                    "next_stage_false": None,
                    "on_error": None
                }
            }
        }
        default_response_json_string = json.dumps(default_plan_dict)
        logger.info(f"MockLLMProvider returning default plan JSON string: {default_response_json_string[:150]}...")
        return default_response_json_string

    def get_last_call_args(self) -> Optional[Dict[str, Any]]:
        return self.calls[-1] if self.calls else None


class LiteLLMProvider(LLMProvider):
    """
    An LLMProvider implementation that uses LiteLLM to connect to various LLM APIs,
    including OpenAI, Anthropic, Google, Ollama, and HuggingFace-compatible endpoints.
    """
    def __init__(self, 
                 default_model: str, 
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 timeout: Optional[int] = None,
                 max_retries: Optional[int] = None,
                 provider_env_vars: Optional[Dict[str, str]] = None,
                 enable_full_logging: Optional[bool] = None):
        if not litellm:
            raise ImportError("LiteLLM library is not installed or failed to import. Cannot use LiteLLMProvider.")

        self.default_model = default_model
        self.api_key = api_key
        self.base_url = base_url
        self.timeout = timeout
        self.max_retries = max_retries
        
        # Enable full logging for refinement debugging
        self.enable_full_logging = enable_full_logging if enable_full_logging is not None else os.getenv("CHUNGOID_FULL_LLM_LOGGING", "false").lower() == "true"
        
        # Store original environment state for potential restoration if needed, or manage carefully.
        # For now, assume direct modification is acceptable for the process lifetime.
        if provider_env_vars:
            logger.info(f"LiteLLMProvider: Setting provider-specific environment variables: {provider_env_vars.keys()}")
            for k, v in provider_env_vars.items():
                os.environ[k] = v
        
        logger.info(f"LiteLLMProvider initialized with default model: {self.default_model}")
        if self.enable_full_logging:
            logger.info("LiteLLMProvider: Full logging enabled for refinement debugging")
        if self.base_url:
            logger.info(f"LiteLLMProvider: Using custom base_url: {self.base_url}")
        if self.api_key:
            logger.info(f"LiteLLMProvider: API key provided directly (will be used if specific env var not found by LiteLLM).")
        if self.timeout:
            logger.info(f"LiteLLMProvider: Default timeout set to {self.timeout} seconds")
        if self.max_retries:
            logger.info(f"LiteLLMProvider: Default max_retries set to {self.max_retries}")


    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = 0.7,
        max_tokens: Optional[int] = 2048,
        system_prompt: Optional[str] = None,
        response_format: Optional[Dict[str, str]] = None,
        **kwargs: Any
    ) -> str:
        if not litellm: # Should have been caught in __init__, but as a safeguard
            raise ImportError("LiteLLM library is not available. Cannot make LLM calls.")

        chosen_model = model_id or self.default_model
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        litellm_kwargs = {
            "model": chosen_model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs 
        }

        # LiteLLM generally prefers API keys in environment variables.
        # Only pass api_key or api_base if they are explicitly provided to this provider instance.
        # LiteLLM will try to auto-detect from environment (e.g. OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.)
        if self.api_key:
            litellm_kwargs["api_key"] = self.api_key
        
        if self.base_url:
            litellm_kwargs["api_base"] = self.base_url

        # Add timeout and retry configuration if available (allow per-call override via kwargs)
        if self.timeout is not None and "timeout" not in kwargs:
            litellm_kwargs["timeout"] = self.timeout
            
        if self.max_retries is not None and "num_retries" not in kwargs:
            # LiteLLM uses 'num_retries' parameter name, not 'max_retries'
            litellm_kwargs["num_retries"] = self.max_retries

        if response_format: # For OpenAI and compatible models that support it
            # Handle different response format types for LiteLLM compatibility
            if isinstance(response_format, dict) and response_format.get("type") == "json_object":
                # LiteLLM may not support the OpenAI-style response_format directly
                # Instead, we'll rely on prompt engineering and post-processing
                logger.debug("LiteLLMProvider: json_object response_format requested, will rely on prompt engineering")
                # Don't pass response_format to LiteLLM, handle via prompt and post-processing
                pass
            else:
                # For other response formats, try to pass them through
                litellm_kwargs["response_format"] = response_format

        try:
            logger.info(f"LiteLLMProvider calling model: {chosen_model} via LiteLLM (prompt first 100 chars): {prompt[:100]}...")
            
            # Add comprehensive debug logging for refinement debugging
            logger.debug(f"LiteLLMProvider FULL PROMPT for model {chosen_model}:")
            logger.debug("=" * 80)
            logger.debug(prompt)
            logger.debug("=" * 80)
            
            # Enhanced logging for refinement debugging
            if self.enable_full_logging:
                logger.info(f"[REFINEMENT DEBUG] LiteLLMProvider FULL PROMPT for model {chosen_model}:")
                logger.info("=" * 80)
                logger.info(prompt)
                logger.info("=" * 80)
            
            logger.debug(f"LiteLLMProvider acompletion kwargs: { {k:v for k,v in litellm_kwargs.items() if k != 'messages'} }")
            
            # Make the LLM call
            response = await litellm.acompletion(**litellm_kwargs)
            
            # Extract content from response
            current_content = response.choices[0].message.content
            
            # Enhanced response validation and logging
            if current_content is None:
                logger.warning(f"[JSON DEBUG] LLM returned None content for model {chosen_model}")
                logger.warning(f"[JSON DEBUG] Full response object: {response}")
                return ""
            
            if not current_content or not current_content.strip():
                logger.warning(f"[JSON DEBUG] LLM returned empty content for model {chosen_model}")
                logger.warning(f"[JSON DEBUG] Content: '{current_content}'")
                logger.warning(f"[JSON DEBUG] Response metadata: usage={getattr(response, 'usage', 'N/A')}, model={getattr(response, 'model', 'N/A')}")
                return ""
            
            # Add comprehensive debug logging for refinement debugging
            logger.debug(f"LiteLLMProvider FULL RESPONSE from model {chosen_model}:")
            logger.debug("=" * 80)
            logger.debug(current_content)
            logger.debug("=" * 80)
            
            # Enhanced logging for refinement debugging
            if self.enable_full_logging:
                logger.info(f"[REFINEMENT DEBUG] LiteLLMProvider FULL RESPONSE from model {chosen_model}:")
                logger.info("=" * 80)
                logger.info(current_content)
                logger.info("=" * 80)
            else:
                # Always show response preview for JSON debugging
                logger.info(f"LiteLLMProvider returning content (first 200 chars after all processing):\n{current_content[:200]}...")
                
                # Add JSON validation preview
                if current_content.strip().startswith('{') or '```json' in current_content:
                    logger.info(f"[JSON DEBUG] Response appears to contain JSON content")
                else:
                    logger.warning(f"[JSON DEBUG] Response does not appear to contain JSON (starts with: '{current_content[:50]}...')")
            
            return current_content

        except Exception as e:
            # Check for LiteLLM specific exceptions if they offer more context
            # from litellm.exceptions import APIConnectionError, RateLimitError, etc.
            logger.error(f"Error calling model {chosen_model} via LiteLLM: {e}", exc_info=True)
            # Consider specific error handling strategy here. Re-raising for now.
            raise

    def _get_expected_api_key_env_var(self, model_name: str) -> Optional[str]:
        """Helper to determine the typical env var for an API key based on model name.
           This is mostly for informational purposes or if direct env var checks were needed,
           as LiteLLM handles most of this internally.
        """
        model_lower = model_name.lower()
        if "openai/" in model_lower or "gpt-" in model_lower or model_lower.startswith("openai"):
            return "OPENAI_API_KEY"
        elif "anthropic/" in model_lower or "claude-" in model_lower or model_lower.startswith("anthropic"):
            return "ANTHROPIC_API_KEY"
        elif "gemini" in model_lower or "google/" in model_lower or model_lower.startswith("google"):
            return "GOOGLE_API_KEY"
        elif "ollama/" in model_lower or model_lower.startswith("ollama"):
            return None # Ollama typically doesn't use API keys
        elif "huggingface/" in model_lower or "hf/" in model_lower: # For HF Inference Endpoints
            return "HF_TOKEN" 
        # Add more mappings as needed
        return None

    async def close_client(self):
        # LiteLLM doesn't typically require explicit client closing in the same way
        # individual SDKs like OpenAI's AsyncClient might.
        # If specific LiteLLM cleanup is needed in the future, it can be added here.
        logger.info(f"LiteLLMProvider: No explicit client close operation typically needed for LiteLLM itself.")
        pass


class LLMManager:
    """
    Manages interactions with an LLM, leveraging a PromptManager for structured prompts
    and an underlying LLMProvider instance for actual API calls.
    """

    def __init__(self, llm_config: Dict[str, Any], prompt_manager: PromptManager):
        """
        Initializes the LLMManager based on a configuration dictionary.

        Args:
            llm_config: Configuration for the LLM provider. Example:
                        {
                            "provider_type": "litellm", # "litellm", "mock", or specific "openai" (legacy)
                            "default_model": "ollama/codellama", 
                            "api_key": "sk-...", # Optional: For direct passthrough if not in env
                            "base_url": "http://localhost:11434", # For Ollama or custom OpenAI-like servers
                            "provider_env_vars": {"CUSTOM_ENV_VAR": "value"}, # Programmatic env var setting
                            "mock_llm_responses": {"prompt_text": "mock_response"} # For "mock" provider
                        }
            prompt_manager: An instance of PromptManager to load and render prompts.
        """
        load_dotenv() # Ensure .env variables are loaded for provider auto-detection

        # Default LLM configuration for fallback
        default_llm_config = {
            "provider": "openai", # "openai", "mock", or "litellm" (legacy)
            "default_model": "gpt-4o-mini-2024-07-18",  # Modern, cost-effective model
            "api_key": None,
            "base_url": None,
            "timeout": 60,
            "max_retries": 3,
        }
        
        # Merge with provided config
        final_config = {**default_llm_config, **llm_config}
        
        provider = final_config.get("provider", "openai").lower()
        logger.info(f"LLMManager: Initializing with provider='{provider}', default_model='{final_config.get('default_model')}'")
        
        if provider == "openai" or provider == "litellm":
            # Both openai and litellm use LiteLLMProvider
            self._provider = LiteLLMProvider(
                default_model=final_config.get("default_model"),
                api_key=final_config.get("api_key"),
                base_url=final_config.get("base_url"),
                timeout=final_config.get("timeout"),
                max_retries=final_config.get("max_retries"),
                provider_env_vars=final_config.get("provider_env_vars", {}),
                enable_full_logging=final_config.get("enable_full_logging"),
            )
            logger.info(f"LLMManager: Using LiteLLMProvider for provider '{provider}'")
        elif provider == "mock":
            # Mock provider for testing
            mock_responses = final_config.get("mock_llm_responses", {})
            self._provider = MockLLMProvider(predefined_responses=mock_responses)
            logger.info("LLMManager: Using MockLLMProvider")
        else:
            logger.warning(f"Unknown LLM provider: '{provider}'. Attempting to use LiteLLMProvider as a fallback.")
            self._provider = LiteLLMProvider(
                default_model=final_config.get("default_model"),
                api_key=final_config.get("api_key"),
                base_url=final_config.get("base_url"),
                timeout=final_config.get("timeout"),
                max_retries=final_config.get("max_retries"),
                provider_env_vars=final_config.get("provider_env_vars", {}),
                enable_full_logging=final_config.get("enable_full_logging"),
            )
        
        self.prompt_manager = prompt_manager
        logger.info(f"LLMManager initialized with concrete provider: {type(self._provider).__name__} and PromptManager.")

    @property
    def actual_provider(self) -> LLMProvider:
        """Returns the actual underlying LLMProvider instance (e.g., LiteLLMProvider, MockLLMProvider)."""
        return self._provider

    async def generate_text_async_with_prompt_manager(
        self,
        prompt_name: str,
        prompt_version: str,
        prompt_render_data: Dict[str, Any],
        prompt_sub_path: Optional[str] = None,
        project_id: Optional[str] = None, 
        calling_agent_id: Optional[str] = None, 
        expected_json_schema: Optional[Type[T]] = None,
        temperature: Optional[float] = None, 
        model_id: Optional[str] = None
    ) -> Any:
        """
        Generates text using a specified prompt from the PromptManager and an underlying LLMProvider.
        Handles fetching prompt definitions, rendering, calling the LLM, and basic JSON validation.
        """
        if not self._provider: # Should be caught by __init__
            logger.error("LLM provider not initialized within LLMManager.")
            raise ValueError("LLMManager cannot make API calls: LLMProvider instance not configured.")

        try:
            prompt_definition: PromptDefinition = self.prompt_manager.get_prompt_definition(
                prompt_name=prompt_name,
                prompt_version=prompt_version,
                prompt_sub_path=prompt_sub_path,
                project_id=project_id,
                calling_agent_id=calling_agent_id,
            )
        except PromptLoadError as e:
            logger.error(f"Failed to load prompt definition for {prompt_name} v{prompt_version}: {e}")
            raise

        try:
            rendered_user_prompt = self.prompt_manager.render_prompt_template_content(
                template_content=prompt_definition.user_prompt_template_content,
                data=prompt_render_data
            )
            system_prompt_content = prompt_definition.system_prompt_template_content
            if system_prompt_content:
                 system_prompt_content = self.prompt_manager.render_prompt_template_content(
                    template_content=system_prompt_content,
                    data=prompt_render_data
                )
        except PromptRenderError as e:
            logger.error(f"Failed to render prompt {prompt_name} v{prompt_version}: {e}")
            raise

        final_model_id = model_id if model_id is not None else prompt_definition.model_config.model_id
        final_temperature = temperature if temperature is not None else prompt_definition.model_config.temperature
        final_max_tokens = prompt_definition.model_config.max_tokens
        
        # Response format from prompt definition should be passed to the provider
        final_response_format = prompt_definition.model_config.response_format

        provider_kwargs = prompt_definition.model_config.provider_specific_params or {}
        # Remove response_format from provider_kwargs if it's handled by the main `response_format` param
        if 'response_format' in provider_kwargs and final_response_format:
            del provider_kwargs['response_format']


        logger.info(f"LLMManager: Calling underlying LLM provider ({type(self._provider).__name__}) for prompt: {prompt_name} v{prompt_version}")
        logger.debug(
            f"LLMManager call details: model_id='{final_model_id}', temperature={final_temperature}, "
            f"max_tokens={final_max_tokens}, system_prompt_present={bool(system_prompt_content)}, "
            f"response_format_from_prompt={final_response_format}, provider_kwargs={provider_kwargs}"
        )

        try:
            llm_output_content = await self._provider.generate(
                prompt=rendered_user_prompt,
                model_id=final_model_id,
                temperature=final_temperature,
                max_tokens=final_max_tokens,
                system_prompt=system_prompt_content, 
                response_format=final_response_format, # Pass the response_format from prompt_definition
                **provider_kwargs 
            )
            
            if not llm_output_content and llm_output_content != "": # Allow empty string as a valid output
                logger.warning(f"LLM call for prompt {prompt_name} v{prompt_version} returned None content.")
                if expected_json_schema:
                    raise ValueError("LLM returned None content but a JSON schema was expected.")
                # If not expecting JSON, None might be problematic too, but "" is fine.
                # For safety, let's treat None as an issue if any output is expected.
                raise ValueError("LLM returned None content.")


            if expected_json_schema:
                try:
                    # llm_output_content should be cleaned by the provider if it's JSON
                    parsed_json = json.loads(llm_output_content)
                    validated_data = expected_json_schema(**parsed_json)
                    logger.info(f"Successfully parsed and validated LLM JSON output against {expected_json_schema.__name__} for prompt {prompt_name} v{prompt_version}.")
                    return validated_data
                except json.JSONDecodeError as e_json:
                    logger.error(f"JSONDecodeError for prompt {prompt_name} v{prompt_version}. LLM Output (first 500 chars): {llm_output_content[:500]}. Error: {e_json}")
                    raise ValueError(f"LLM output was not valid JSON. Error: {e_json}. Output: {llm_output_content[:200]}...") from e_json
                except ValidationError as e_val:
                    logger.error(f"Pydantic ValidationError for prompt {prompt_name} v{prompt_version}. LLM Output (first 500 chars): {llm_output_content[:500]}. Error: {e_val}")
                    raise ValueError(f"LLM output did not match expected schema {expected_json_schema.__name__}. Error: {e_val}. Output: {llm_output_content[:200]}...") from e_val
                except Exception as e_parse: 
                    logger.error(f"Unexpected error parsing/validating LLM output for prompt {prompt_name} v{prompt_version}. Error: {e_parse}. Output: {llm_output_content[:500]}")
                    raise ValueError(f"Failed to process LLM output against schema. Error: {e_parse}") from e_parse
            else:
                logger.info(f"LLM call successful for prompt {prompt_name} v{prompt_version}. Returning raw text output.")
                return llm_output_content

        # Catching LiteLLM's specific exceptions can be useful here if finer-grained error handling is needed.
        # Example:
        # except litellm.RateLimitError as e_rate_limit:
        #    logger.error(f"LiteLLM RateLimitError for prompt {prompt_name} v{prompt_version}: {e_rate_limit}")
        #    raise
        # except litellm.APIConnectionError as e_conn:
        #    logger.error(f"LiteLLM APIConnectionError for prompt {prompt_name} v{prompt_version}: {e_conn}")
        #    raise
        except Exception as e_call: # General catch-all
            logger.error(f"Generic Exception during LLM call for prompt {prompt_name} v{prompt_version} via provider {type(self._provider).__name__}: {e_call}", exc_info=True)
            raise 

    async def close_client(self):
        if self._provider and hasattr(self._provider, 'close_client'):
            try:
                await self._provider.close_client()
                logger.info(f"LLMManager: Underlying LLM provider ({type(self._provider).__name__}) client closed.")
            except Exception as e:
                logger.error(f"LLMManager: Error closing underlying LLM provider client: {e}", exc_info=True)
        else:
            logger.info(f"LLMManager: Underlying LLM provider ({type(self._provider).__name__}) does not have a close_client method or provider not set.")

# Removed OpenAILLMProvider class as LiteLLMProvider is intended to replace it.
# If OpenAILLMProvider needs to be kept for some specific reason, it can be reinstated,
# but the goal is to primarily use LiteLLMProvider.

# Example usage (conceptual, not runnable here)
# async def main():
#     # Setup PromptManager (assuming it's configured correctly)
#     pm = PromptManager(prompt_directory_paths=["path/to/prompts"])
#     # Create an instance of a concrete LLMProvider (ABC compliant)
#     concrete_provider = OpenAILLMProvider(api_key="your_openai_api_key") 
#     llm_manager = LLMManager(llm_provider_instance=concrete_provider, prompt_manager=pm)
#
#     if not concrete_provider.client: # Adjust check based on concrete_provider's attributes
#          print("Concrete provider not configured (e.g. API Key). Exiting.")
#          return
#
#     try:
#         response = await llm_manager.generate_text_async_with_prompt_manager(
#             prompt_name="example_prompt",
#             prompt_version="v1",
#             prompt_render_data={"user_name": "TestUser"},
#             # expected_json_schema=MyExpectedPydanticModel # Optional
#         )
#         print("LLM Response:", response)
#     except Exception as e:
#         print(f"An error occurred: {e}")
#     finally:
#         await llm_manager.close_client() # This will call concrete_provider.close_client()

# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main()) 