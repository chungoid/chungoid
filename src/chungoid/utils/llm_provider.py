from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, List, Type, TypeVar
import logging
import uuid
import json
import os

from pydantic import BaseModel, ValidationError
from openai import AsyncOpenAI, APIError
from dotenv import load_dotenv

from chungoid.utils.prompt_manager import PromptManager, PromptDefinition, PromptRenderError, PromptLoadError

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseModel)

class LLMProvider(ABC):
    """
    Abstract Base Class for an LLM interaction provider.
    Defines a standard interface for sending prompts to an LLM and receiving responses.
    """

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        # TODO: Consider adding stop_sequences, system_prompt (if not part of main prompt)
        **kwargs: Any  # For provider-specific parameters
    ) -> str:
        """
        Sends a prompt to the LLM and returns the generated text response.

        Args:
            prompt: The main prompt string for the LLM.
            model_id: Optional; specific model identifier if overriding a default.
            temperature: Optional; sampling temperature.
            max_tokens: Optional; maximum number of tokens to generate.
            kwargs: Additional provider-specific arguments.

        Returns:
            The LLM's generated text response.
        """
        pass


class MockLLMProvider(LLMProvider):
    """
    A mock implementation of LLMProvider for testing and development.
    It can be configured to return predefined responses or echo prompts.
    """

    def __init__(self, predefined_responses: Optional[Dict[str, str]] = None):
        self.predefined_responses = predefined_responses if predefined_responses else {}
        # Track calls for testing purposes
        self.calls: List[Dict[str, Any]] = []

    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs: Any
    ) -> str:
        self.calls.append({
            "prompt": prompt,
            "model_id": model_id,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "kwargs": kwargs
        })

        logger.info(f"MockLLMProvider received prompt (first 100 chars): {prompt[:100]}...")
        logger.info(f"MockLLMProvider args: model_id={model_id}, temp={temperature}, max_tokens={max_tokens}")

        # Check if a direct match for the prompt exists in predefined_responses
        if prompt in self.predefined_responses:
            response = self.predefined_responses[prompt]
            logger.info(f"MockLLMProvider returning predefined response for direct prompt match: {response}")
            return response

        # Check for partial matches (e.g., if prompt starts with a key)
        for key, predefined_response in self.predefined_responses.items():
            if prompt.strip().startswith(key.strip()):
                logger.info(f"MockLLMProvider returning predefined response for partial match (key: '{key}'): {predefined_response}")
                return predefined_response
        
        # Default behavior: return a stringified minimal valid MasterExecutionPlan
        plan_id_mock = f"mock_plan_{str(uuid.uuid4())[:4]}" # Keep uuid import for this if not already present at file top
        default_plan_dict = {
            "id": plan_id_mock,
            "name": f"Mock Plan for {prompt[:50]}...",
            "description": "A minimal mock plan generated by MockLLMProvider.",
            "start_stage": "mock_stage_1",
            "stages": {
                "mock_stage_1": {
                    "number": 1,
                    "name": "Mock Stage 1",
                    "description": "This is a mock stage.",
                    "agent_id": "HumanInputAgent_v1", # A known safe agent
                    "inputs": {"message": "Mock input for stage 1"},
                    "next_stage": "FINAL_STEP"
                }
            }
        }
        default_response_json_string = json.dumps(default_plan_dict) # Keep json import for this
        logger.info(f"MockLLMProvider returning default plan JSON string: {default_response_json_string[:150]}...")
        return default_response_json_string

    def get_last_call_args(self) -> Optional[Dict[str, Any]]:
        return self.calls[-1] if self.calls else None


class OpenAILLMProvider(LLMProvider):
    """
    An LLMProvider implementation that connects to OpenAI's API.
    """
    def __init__(self, api_key: str, default_model: str = "gpt-3.5-turbo"):
        try:
            import openai # type: ignore
        except ImportError:
            logger.error("OpenAI library is not installed. Please install it with `pip install openai`.")
            raise
        
        if not api_key:
            logger.error("OpenAI API key is required for OpenAILLMProvider.")
            raise ValueError("OpenAI API key cannot be empty.")
            
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.default_model = default_model
        logger.info(f"OpenAILLMProvider initialized with default model: {self.default_model}")

    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = 0.7, # Default temperature for OpenAI
        max_tokens: Optional[int] = 2048,   # A reasonable default max_tokens
        system_prompt: Optional[str] = None, # Allow for an optional system prompt
        **kwargs: Any
    ) -> str:
        chosen_model = model_id or self.default_model
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            logger.info(f"OpenAILLMProvider calling model: {chosen_model} with prompt (first 100 chars): {prompt[:100]}...")
            
            response = await self.client.chat.completions.create(
                model=chosen_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format={"type": "json_object"}, # Ensure JSON output
                **kwargs
            )
            
            generated_content = response.choices[0].message.content
            
            # Log the raw response from OpenAI
            logger.info(f"OpenAILLMProvider RAW response content from {chosen_model}:\\\\n---BEGIN RAW RESPONSE---\\\\n{generated_content}\\\\n---END RAW RESPONSE---")

            if generated_content is None:
                logger.warning("OpenAI returned None content.")
                return "" # Return empty string if content is None

            # Strip Markdown fences if present
            cleaned_content = generated_content.strip()
            if cleaned_content.startswith("```json"):
                cleaned_content = cleaned_content[7:] # Remove ```json
            elif cleaned_content.startswith("```"):
                cleaned_content = cleaned_content[3:] # Remove ```
            
            if cleaned_content.endswith("```"):
                cleaned_content = cleaned_content[:-3] # Remove ```
            
            cleaned_content = cleaned_content.strip() # Strip any leading/trailing whitespace again

            logger.info(f"OpenAILLMProvider CLEANED response content:\\\\n---BEGIN CLEANED RESPONSE---\\\\n{cleaned_content}\\\\n---END CLEANED RESPONSE---")
            return cleaned_content

        except Exception as e:
            logger.error(f"Error calling OpenAI API (model: {chosen_model}): {e}")
            # Consider specific error handling for API errors (e.g., rate limits, auth)
            # For now, re-raise to allow the caller to handle or log more details.
            # Alternatively, could return a specific error message string.
            raise # Re-raise the exception

# Example conceptual code that was previously here has been removed as we're adding the concrete implementation.

# Example of how a concrete implementation might look (conceptual)
# class OpenAILLMProvider(LLMProvider):
#     def __init__(self, api_key: str, default_model: str = "gpt-3.5-turbo"):
#         import openai # type: ignore
#         self.client = openai.AsyncOpenAI(api_key=api_key)
#         self.default_model = default_model

#     async def generate(
#         self,
#         prompt: str,
#         model_id: Optional[str] = None,
#         temperature: Optional[float] = 0.7, # Default temperature
#         max_tokens: Optional[int] = 1500,   # Default max_tokens
#         **kwargs: Any
#     ) -> str:
#         chosen_model = model_id or self.default_model
#         try:
#             logger.info(f"OpenAILLMProvider calling model: {chosen_model} with prompt (first 100 chars): {prompt[:100]}...")
#             # Note: OpenAI API might prefer a messages format for chat models
#             # This is a simplified example for completion-style interaction.
#             # For chat models, you'd construct a messages list:
#             # messages = [{"role": "user", "content": prompt}]
#             # response = await self.client.chat.completions.create(
#             #     model=chosen_model,
#             #     messages=messages,
#             #     temperature=temperature,
#             #     max_tokens=max_tokens,
#             #     **kwargs
#             # )
#             # return response.choices[0].message.content or ""
            
#             # Using a more generic completion for this example, assuming `prompt` is a full prompt
#             # This might vary significantly based on the chosen model and its capabilities.
#             # The actual implementation needs to align with the specific OpenAI client library and model type.
            
#             # Placeholder for actual OpenAI call which might be different
#             # For instance, for newer models, it's often chat completions.
#             # This is illustrative.
#             if "chat.completions" in chosen_model: # A guess
#                 response = await self.client.chat.completions.create(
#                     model=chosen_model,
#                     messages=[{"role": "system", "content": "You are a helpful assistant."}, # Or use a passed system_prompt
#                               {"role": "user", "content": prompt}],
#                     temperature=temperature,
#                     max_tokens=max_tokens,
#                     **kwargs
#                 )
#                 return response.choices[0].message.content or ""
#             else: # Fallback for older completion models, if any
#                 response = await self.client.completions.create(
#                     model=chosen_model,
#                     prompt=prompt,
#                     temperature=temperature,
#                     max_tokens=max_tokens,
#                     **kwargs
#                 )
#                 return response.choices[0].message.content or "" # Ensure it returns string


#         except Exception as e:
#             logger.error(f"Error calling OpenAI API: {e}")
#             raise


class LLMManager: # RENAMED from LLMProvider
    """
    Manages interactions with an LLM, leveraging a PromptManager for structured prompts
    and an underlying LLMProvider instance for actual API calls.
    This class is responsible for orchestrating prompt rendering, making the LLM call
    via the provided provider, and potentially parsing the response.
    """

    def __init__(self, llm_provider_instance: LLMProvider, prompt_manager: PromptManager):
        """
        Initializes the LLMManager.

        Args:
            llm_provider_instance: An instance of a class that implements the LLMProvider ABC 
                                   (e.g., OpenAILLMProvider, MockLLMProvider). This instance
                                   will be used for the actual LLM API calls.
            prompt_manager: An instance of PromptManager to load and render prompts.
        """
        # load_dotenv() # Dotenv loading should ideally be handled at app entry or by specific providers
        # self.openai_api_key = os.getenv("OPENAI_API_KEY") # API key management is now handled by llm_provider_instance
        # if not self.openai_api_key:
        #     logger.warning("OPENAI_API_KEY not found in environment variables if direct OpenAI calls were intended here.")
        # self.client = AsyncOpenAI(api_key=self.openai_api_key) # OpenAI client is now managed by llm_provider_instance (e.g. OpenAILLMProvider)
        
        self._llm_provider = llm_provider_instance
        self.prompt_manager = prompt_manager
        logger.info(f"LLMManager initialized with provider: {type(llm_provider_instance).__name__} and PromptManager.")

    async def generate_text_async_with_prompt_manager(
        self,
        prompt_name: str,
        prompt_version: str,
        prompt_render_data: Dict[str, Any],
        prompt_sub_path: Optional[str] = None,
        project_id: Optional[str] = None, 
        calling_agent_id: Optional[str] = None, 
        expected_json_schema: Optional[Type[T]] = None,
        temperature: Optional[float] = None, # Allow overriding prompt-defined temperature
        model_id: Optional[str] = None      # Allow overriding prompt-defined model
    ) -> Any:
        """
        Generates text using a specified prompt from the PromptManager and an underlying LLMProvider.
        Handles fetching prompt definitions, rendering, calling the LLM, and basic JSON validation.
        """
        if not self._llm_provider:
            logger.error("LLM provider not initialized within LLMManager.")
            raise ValueError(
                "LLMManager cannot make API calls: LLMProvider instance not configured."
            )

        try:
            prompt_definition: PromptDefinition = self.prompt_manager.get_prompt_definition(
                prompt_name=prompt_name,
                prompt_version=prompt_version,
                prompt_sub_path=prompt_sub_path,
                project_id=project_id,
                calling_agent_id=calling_agent_id,
            )
        except PromptLoadError as e:
            logger.error(f"Failed to load prompt definition for {prompt_name} v{prompt_version}: {e}")
            raise

        try:
            # Render the user prompt (main content)
            rendered_user_prompt = self.prompt_manager.render_prompt_template_content(
                template_content=prompt_definition.user_prompt_template_content,
                data=prompt_render_data
            )
            # System prompt is taken directly from the definition (it might be None)
            system_prompt_content = prompt_definition.system_prompt_template_content
            if system_prompt_content: # Render if it has variables
                 system_prompt_content = self.prompt_manager.render_prompt_template_content(
                    template_content=system_prompt_content,
                    data=prompt_render_data # Assuming system prompt might also use render data
                )

        except PromptRenderError as e:
            logger.error(f"Failed to render prompt {prompt_name} v{prompt_version}: {e}")
            raise

        # Determine parameters for the underlying LLMProvider call
        # Use overrides if provided, otherwise use values from prompt_definition.model_config
        final_model_id = model_id if model_id is not None else prompt_definition.model_config.model_id
        final_temperature = temperature if temperature is not None else prompt_definition.model_config.temperature
        final_max_tokens = prompt_definition.model_config.max_tokens # No direct override for max_tokens in this method's signature yet

        # Prepare kwargs for provider-specific parameters from model_config, excluding known ones
        provider_kwargs = prompt_definition.model_config.provider_specific_params or {}

        # Ensure response_format is passed if defined in prompt (OpenAILLMProvider handles this)
        if prompt_definition.model_config.response_format:
            provider_kwargs['response_format'] = prompt_definition.model_config.response_format
        
        logger.info(f"LLMManager: Calling underlying LLM provider ({type(self._llm_provider).__name__}) for prompt: {prompt_name} v{prompt_version}")
        logger.debug(
            f"LLMManager call details: model_id='{final_model_id}', temperature={final_temperature}, "
            f"max_tokens={final_max_tokens}, system_prompt_present={bool(system_prompt_content)}, "
            f"provider_kwargs={provider_kwargs}"
        )
        # logger.debug(f"LLMManager User Prompt (first 200 chars): {rendered_user_prompt[:200]}") # Can be verbose

        try:
            llm_output_content = await self._llm_provider.generate(
                prompt=rendered_user_prompt,
                model_id=final_model_id,
                temperature=final_temperature,
                max_tokens=final_max_tokens,
                system_prompt=system_prompt_content, 
                **provider_kwargs 
            )
            
            if not llm_output_content:
                logger.warning(f"LLM call for prompt {prompt_name} v{prompt_version} returned empty or None content.")
                # Depending on expected_json_schema, might need to raise or return default
                if expected_json_schema:
                    raise ValueError("LLM returned empty content but a JSON schema was expected.")
                return "" # Return empty string if no schema expected and content is empty

            # Attempt to parse if a JSON schema is expected
            if expected_json_schema:
                try:
                    # The llm_output_content should already be cleaned by OpenAILLMProvider if it was that one.
                    # Other providers might need cleaning here.
                    # For now, assume llm_output_content is a string ready for json.loads.
                    parsed_json = json.loads(llm_output_content)
                    validated_data = expected_json_schema(**parsed_json)
                    logger.info(f"Successfully parsed and validated LLM JSON output against {expected_json_schema.__name__} for prompt {prompt_name} v{prompt_version}.")
                    return validated_data
                except json.JSONDecodeError as e_json:
                    logger.error(f"JSONDecodeError for prompt {prompt_name} v{prompt_version}. LLM Output (first 500 chars): {llm_output_content[:500]}. Error: {e_json}")
                    raise ValueError(f"LLM output was not valid JSON. Error: {e_json}. Output: {llm_output_content[:200]}...") from e_json
                except ValidationError as e_val:
                    logger.error(f"Pydantic ValidationError for prompt {prompt_name} v{prompt_version}. LLM Output (first 500 chars): {llm_output_content[:500]}. Error: {e_val}")
                    raise ValueError(f"LLM output did not match expected schema {expected_json_schema.__name__}. Error: {e_val}. Output: {llm_output_content[:200]}...") from e_val
                except Exception as e_parse: # Catch any other parsing/validation error
                    logger.error(f"Unexpected error parsing/validating LLM output for prompt {prompt_name} v{prompt_version}. Error: {e_parse}. Output: {llm_output_content[:500]}")
                    raise ValueError(f"Failed to process LLM output against schema. Error: {e_parse}") from e_parse
            else:
                # Return raw text if no JSON schema is expected
                logger.info(f"LLM call successful for prompt {prompt_name} v{prompt_version}. Returning raw text output.")
                return llm_output_content

        except APIError as e: # Specific error from OpenAI library if that's the provider
            logger.error(f"OpenAI APIError during LLM call for prompt {prompt_name} v{prompt_version}: {e}")
            raise # Re-raise APIError to be handled by caller
        except Exception as e_call:
            logger.error(f"Generic Exception during LLM call for prompt {prompt_name} v{prompt_version} via provider {type(self._llm_provider).__name__}: {e_call}", exc_info=True)
            raise # Re-raise generic exception

    # Consider if close_client is needed at this manager level,
    # or if it should be managed by the owner of the llm_provider_instance.
    # If LLMManager creates the provider, it should close it. If it receives it, the creator should close it.
    # For now, providing a pass-through.
    async def close_client(self):
        if self._llm_provider and hasattr(self._llm_provider, 'close_client'):
            try:
                # Assuming close_client is an async method on the provider
                await self._llm_provider.close_client()
                logger.info(f"LLMManager: Underlying LLM provider ({type(self._llm_provider).__name__}) client closed.")
            except Exception as e:
                logger.error(f"LLMManager: Error closing underlying LLM provider client: {e}", exc_info=True)
        else:
            logger.info(f"LLMManager: Underlying LLM provider ({type(self._llm_provider).__name__}) does not have a close_client method or provider not set.")

# Example usage (conceptual, not runnable here)
# async def main():
#     # Setup PromptManager (assuming it's configured correctly)
#     pm = PromptManager(prompt_directory_paths=["path/to/prompts"])
#     # Create an instance of a concrete LLMProvider (ABC compliant)
#     concrete_provider = OpenAILLMProvider(api_key="your_openai_api_key") 
#     llm_manager = LLMManager(llm_provider_instance=concrete_provider, prompt_manager=pm)
#
#     if not concrete_provider.client: # Adjust check based on concrete_provider's attributes
#          print("Concrete provider not configured (e.g. API Key). Exiting.")
#          return
#
#     try:
#         response = await llm_manager.generate_text_async_with_prompt_manager(
#             prompt_name="example_prompt",
#             prompt_version="v1",
#             prompt_render_data={"user_name": "TestUser"},
#             # expected_json_schema=MyExpectedPydanticModel # Optional
#         )
#         print("LLM Response:", response)
#     except Exception as e:
#         print(f"An error occurred: {e}")
#     finally:
#         await llm_manager.close_client() # This will call concrete_provider.close_client()

# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main()) 