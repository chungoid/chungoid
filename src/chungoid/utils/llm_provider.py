from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, List, Type, TypeVar
import logging
import uuid
import json
import os

from pydantic import BaseModel, ValidationError
from openai import AsyncOpenAI, APIError
from dotenv import load_dotenv

from chungoid.utils.prompt_manager import PromptManager, PromptDefinition, PromptRenderError, PromptLoadError

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseModel)

class LLMProvider(ABC):
    """
    Abstract Base Class for an LLM interaction provider.
    Defines a standard interface for sending prompts to an LLM and receiving responses.
    """

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        # TODO: Consider adding stop_sequences, system_prompt (if not part of main prompt)
        **kwargs: Any  # For provider-specific parameters
    ) -> str:
        """
        Sends a prompt to the LLM and returns the generated text response.

        Args:
            prompt: The main prompt string for the LLM.
            model_id: Optional; specific model identifier if overriding a default.
            temperature: Optional; sampling temperature.
            max_tokens: Optional; maximum number of tokens to generate.
            kwargs: Additional provider-specific arguments.

        Returns:
            The LLM's generated text response.
        """
        pass


class MockLLMProvider(LLMProvider):
    """
    A mock implementation of LLMProvider for testing and development.
    It can be configured to return predefined responses or echo prompts.
    """

    def __init__(self, predefined_responses: Optional[Dict[str, str]] = None):
        self.predefined_responses = predefined_responses if predefined_responses else {}
        # Track calls for testing purposes
        self.calls: List[Dict[str, Any]] = []

    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs: Any
    ) -> str:
        self.calls.append({
            "prompt": prompt,
            "model_id": model_id,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "kwargs": kwargs
        })

        logger.info(f"MockLLMProvider received prompt (first 100 chars): {prompt[:100]}...")
        logger.info(f"MockLLMProvider args: model_id={model_id}, temp={temperature}, max_tokens={max_tokens}")

        # Check if a direct match for the prompt exists in predefined_responses
        if prompt in self.predefined_responses:
            response = self.predefined_responses[prompt]
            logger.info(f"MockLLMProvider returning predefined response for direct prompt match: {response}")
            return response

        # Check for partial matches (e.g., if prompt starts with a key)
        for key, predefined_response in self.predefined_responses.items():
            if prompt.strip().startswith(key.strip()):
                logger.info(f"MockLLMProvider returning predefined response for partial match (key: '{key}'): {predefined_response}")
                return predefined_response
        
        # Default behavior: return a stringified minimal valid MasterExecutionPlan
        plan_id_mock = f"mock_plan_{str(uuid.uuid4())[:4]}" # Keep uuid import for this if not already present at file top
        default_plan_dict = {
            "id": plan_id_mock,
            "name": f"Mock Plan for {prompt[:50]}...",
            "description": "A minimal mock plan generated by MockLLMProvider.",
            "start_stage": "mock_stage_1",
            "stages": {
                "mock_stage_1": {
                    "number": 1,
                    "name": "Mock Stage 1",
                    "description": "This is a mock stage.",
                    "agent_id": "HumanInputAgent_v1", # A known safe agent
                    "inputs": {"message": "Mock input for stage 1"},
                    "next_stage": "FINAL_STEP"
                }
            }
        }
        default_response_json_string = json.dumps(default_plan_dict) # Keep json import for this
        logger.info(f"MockLLMProvider returning default plan JSON string: {default_response_json_string[:150]}...")
        return default_response_json_string

    def get_last_call_args(self) -> Optional[Dict[str, Any]]:
        return self.calls[-1] if self.calls else None


class OpenAILLMProvider(LLMProvider):
    """
    An LLMProvider implementation that connects to OpenAI's API.
    """
    def __init__(self, api_key: str, default_model: str = "gpt-3.5-turbo"):
        try:
            import openai # type: ignore
        except ImportError:
            logger.error("OpenAI library is not installed. Please install it with `pip install openai`.")
            raise
        
        if not api_key:
            logger.error("OpenAI API key is required for OpenAILLMProvider.")
            raise ValueError("OpenAI API key cannot be empty.")
            
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.default_model = default_model
        logger.info(f"OpenAILLMProvider initialized with default model: {self.default_model}")

    async def generate(
        self,
        prompt: str,
        model_id: Optional[str] = None,
        temperature: Optional[float] = 0.7, # Default temperature for OpenAI
        max_tokens: Optional[int] = 2048,   # A reasonable default max_tokens
        system_prompt: Optional[str] = None, # Allow for an optional system prompt
        **kwargs: Any
    ) -> str:
        chosen_model = model_id or self.default_model
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            logger.info(f"OpenAILLMProvider calling model: {chosen_model} with prompt (first 100 chars): {prompt[:100]}...")
            
            response = await self.client.chat.completions.create(
                model=chosen_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format={"type": "json_object"}, # Ensure JSON output
                **kwargs
            )
            
            generated_content = response.choices[0].message.content
            
            # Log the raw response from OpenAI
            logger.info(f"OpenAILLMProvider RAW response content from {chosen_model}:\\\\n---BEGIN RAW RESPONSE---\\\\n{generated_content}\\\\n---END RAW RESPONSE---")

            if generated_content is None:
                logger.warning("OpenAI returned None content.")
                return "" # Return empty string if content is None

            # Strip Markdown fences if present
            cleaned_content = generated_content.strip()
            if cleaned_content.startswith("```json"):
                cleaned_content = cleaned_content[7:] # Remove ```json
            elif cleaned_content.startswith("```"):
                cleaned_content = cleaned_content[3:] # Remove ```
            
            if cleaned_content.endswith("```"):
                cleaned_content = cleaned_content[:-3] # Remove ```
            
            cleaned_content = cleaned_content.strip() # Strip any leading/trailing whitespace again

            logger.info(f"OpenAILLMProvider CLEANED response content:\\\\n---BEGIN CLEANED RESPONSE---\\\\n{cleaned_content}\\\\n---END CLEANED RESPONSE---")
            return cleaned_content

        except Exception as e:
            logger.error(f"Error calling OpenAI API (model: {chosen_model}): {e}")
            # Consider specific error handling for API errors (e.g., rate limits, auth)
            # For now, re-raise to allow the caller to handle or log more details.
            # Alternatively, could return a specific error message string.
            raise # Re-raise the exception

# Example conceptual code that was previously here has been removed as we're adding the concrete implementation.

# Example of how a concrete implementation might look (conceptual)
# class OpenAILLMProvider(LLMProvider):
#     def __init__(self, api_key: str, default_model: str = "gpt-3.5-turbo"):
#         import openai # type: ignore
#         self.client = openai.AsyncOpenAI(api_key=api_key)
#         self.default_model = default_model

#     async def generate(
#         self,
#         prompt: str,
#         model_id: Optional[str] = None,
#         temperature: Optional[float] = 0.7, # Default temperature
#         max_tokens: Optional[int] = 1500,   # Default max_tokens
#         **kwargs: Any
#     ) -> str:
#         chosen_model = model_id or self.default_model
#         try:
#             logger.info(f"OpenAILLMProvider calling model: {chosen_model} with prompt (first 100 chars): {prompt[:100]}...")
#             # Note: OpenAI API might prefer a messages format for chat models
#             # This is a simplified example for completion-style interaction.
#             # For chat models, you'd construct a messages list:
#             # messages = [{"role": "user", "content": prompt}]
#             # response = await self.client.chat.completions.create(
#             #     model=chosen_model,
#             #     messages=messages,
#             #     temperature=temperature,
#             #     max_tokens=max_tokens,
#             #     **kwargs
#             # )
#             # return response.choices[0].message.content or ""
            
#             # Using a more generic completion for this example, assuming `prompt` is a full prompt
#             # This might vary significantly based on the chosen model and its capabilities.
#             # The actual implementation needs to align with the specific OpenAI client library and model type.
            
#             # Placeholder for actual OpenAI call which might be different
#             # For instance, for newer models, it's often chat completions.
#             # This is illustrative.
#             if "chat.completions" in chosen_model: # A guess
#                 response = await self.client.chat.completions.create(
#                     model=chosen_model,
#                     messages=[{"role": "system", "content": "You are a helpful assistant."}, # Or use a passed system_prompt
#                               {"role": "user", "content": prompt}],
#                     temperature=temperature,
#                     max_tokens=max_tokens,
#                     **kwargs
#                 )
#                 return response.choices[0].message.content or ""
#             else: # Fallback for older completion models, if any
#                 response = await self.client.completions.create(
#                     model=chosen_model,
#                     prompt=prompt,
#                     temperature=temperature,
#                     max_tokens=max_tokens,
#                     **kwargs
#                 )
#                 return response.choices[0].text.strip()

#         except Exception as e:
#             logger.error(f"Error calling OpenAI API: {e}")
#             # Consider specific error handling or re-raising
#             raise # Re-raise the exception for the caller to handle

#         return "Error: Could not get response from OpenAI." # Fallback 

class LLMProvider:
    """
    Provides an interface to interact with Large Language Models (LLMs),
    handling prompt management and API communication.
    """

    def __init__(self, prompt_manager: PromptManager):
        """
        Initializes the LLMProvider.

        Args:
            prompt_manager: An instance of PromptManager to retrieve prompt templates.
        """
        self.prompt_manager = prompt_manager
        load_dotenv()  # Load environment variables from .env file

        self._api_key: Optional[str] = os.getenv("OPENAI_API_KEY")
        self._openai_client: Optional[AsyncOpenAI] = None

        if self._api_key:
            self._openai_client = AsyncOpenAI(api_key=self._api_key)
            logger.info("OpenAI client initialized with API key.")
        else:
            logger.warning(
                "OPENAI_API_KEY not found in environment. "
                "LLMProvider will not be able to make live API calls."
            )

    async def generate_text_async_with_prompt_manager(
        self,
        prompt_name: str,
        prompt_version: str,
        prompt_render_data: Dict[str, Any],
        prompt_sub_path: Optional[str] = None,
        project_id: Optional[str] = None, 
        calling_agent_id: Optional[str] = None, 
        expected_json_schema: Optional[Type[T]] = None,
        temperature: Optional[float] = None, # Allow overriding prompt-defined temperature
        model_id: Optional[str] = None      # Allow overriding prompt-defined model
    ) -> Any:
        """
        Generates text using a specified prompt, version, and render data via PromptManager.
        Handles fetching prompt definitions, rendering, calling the LLM, and basic JSON validation.
        """
        if not self._openai_client:
            logger.error("OpenAI client not initialized. Missing API key.")
            raise ValueError(
                "LLMProvider cannot make API calls: OPENAI_API_KEY not configured."
            )

        try:
            prompt_definition = self.prompt_manager.get_prompt_definition(
                prompt_name, prompt_version, sub_path=prompt_sub_path
            )
            if not prompt_definition:
                # This case should ideally be caught by get_prompt_definition raising PromptNotFoundError
                raise ValueError(
                    f"Prompt '{prompt_name}' version '{prompt_version}' (sub_path: {prompt_sub_path}) not found."
                )

            system_prompt_str = self.prompt_manager.get_rendered_prompt_template(
                prompt_definition.system_prompt_template, prompt_render_data
            )
            user_prompt_str = self.prompt_manager.get_rendered_prompt_template(
                prompt_definition.user_prompt_template, prompt_render_data
            )
            
            messages = [
                {"role": "system", "content": system_prompt_str},
                {"role": "user", "content": user_prompt_str},
            ]

            # Determine model, temperature, max_tokens
            # Priority: direct parameter > prompt setting > hardcoded default
            final_model_id = model_id or prompt_definition.model_settings.model_name or "gpt-4-turbo-preview"
            final_temperature = temperature if temperature is not None else prompt_definition.model_settings.temperature # Assumes Pydantic model provides a default
            final_max_tokens = prompt_definition.model_settings.max_tokens or 2048
            
            request_params = {
                "model": final_model_id,
                "messages": messages,
                "max_tokens": final_max_tokens,
                "temperature": final_temperature,
            }

            # Add response_format for JSON mode if applicable
            # Common models supporting JSON mode: gpt-4-turbo, gpt-4-turbo-preview, gpt-3.5-turbo-1106, gpt-3.5-turbo-0125, gpt-4-0125-preview, gpt-4-1106-preview
            json_mode_supported_models = ["gpt-4-turbo", "gpt-4-turbo-preview", "gpt-3.5-turbo-1106", "gpt-3.5-turbo-0125", "gpt-4-0125-preview", "gpt-4-1106-preview"]
            if expected_json_schema and final_model_id in json_mode_supported_models:
                request_params["response_format"] = {"type": "json_object"}
                logger.info(f"Requesting JSON object response format for model {final_model_id}")
            elif expected_json_schema:
                logger.warning(f"Model {final_model_id} may not support JSON object mode, but JSON output is expected. Proceeding without forcing JSON mode.")

            logger.info(
                f"Making OpenAI API call to model: {final_model_id} for prompt: {prompt_name} v{prompt_version}. Temp: {final_temperature}, MaxTokens: {final_max_tokens}"
            )
            # logger.debug(f"OpenAI API request messages: {messages}") # Can be very verbose

            api_response = await self._openai_client.chat.completions.create(**request_params)
            
            llm_output_content = api_response.choices[0].message.content

            if not llm_output_content:
                logger.warning("LLM returned empty content.")
                raise ValueError("LLM returned empty content.")

            # Basic cleaning of common Markdown fences if JSON is expected
            if expected_json_schema:
                cleaned_output = llm_output_content.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[7:].strip()
                    if cleaned_output.endswith("```"):
                        cleaned_output = cleaned_output[:-3].strip()
                elif cleaned_output.startswith("```") and cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[3:-3].strip()
                llm_output_content = cleaned_output

            if expected_json_schema:
                try:
                    parsed_json = json.loads(llm_output_content)
                    validated_output = expected_json_schema(**parsed_json)
                    logger.info(f"LLM response parsed and validated successfully against {expected_json_schema.__name__} for prompt: {prompt_name}")
                    return validated_output
                except json.JSONDecodeError as e_json_decode:
                    logger.error(
                        f"Failed to parse LLM response as JSON for prompt {prompt_name}. Error: {e_json_decode}. LLM output: '{llm_output_content[:500]}...'"
                    )
                    raise # Re-raise the JSONDecodeError
                except ValidationError as e_validation: # Assuming pydantic.ValidationError
                    logger.error(
                        f"LLM JSON response failed Pydantic validation for {expected_json_schema.__name__} for prompt {prompt_name}. Error: {e_validation}. Raw JSON attempted: '{llm_output_content[:500]}...'"
                    )
                    raise # Re-raise the ValidationError
            else:
                logger.info(f"LLM response received successfully as string for prompt: {prompt_name}")
                return llm_output_content
        
        except PromptLoadError as e_prompt_nf:
            logger.error(f"Prompt '{prompt_name}' v'{prompt_version}' not found: {e_prompt_nf}", exc_info=True)
            raise
        except PromptRenderError as e_render:
            logger.error(f"Error rendering prompt '{prompt_name}' v'{prompt_version}': {e_render}", exc_info=True)
            raise
        except APIError as e_api: # Catch OpenAI specific API errors
            logger.error(f"OpenAI API error for prompt '{prompt_name}': {e_api}", exc_info=True)
            raise ValueError(f"OpenAI API error: {e_api}")
        except Exception as e_general: # Catch any other unexpected errors
            logger.error(f"Unexpected error in generate_text_async_with_prompt_manager for prompt '{prompt_name}': {e_general}", exc_info=True)
            raise ValueError(f"Unexpected error during LLM call: {e_general}")

    # Consider adding a method to explicitly close the httpx client used by AsyncOpenAI
    # if running in a context where it's necessary (e.g. specific server frameworks)
    async def close_client(self):
        if self._openai_client:
            await self._openai_client.close()
            logger.info("OpenAI client closed.")

# Example usage (conceptual, not runnable here)
# async def main():
#     # Setup PromptManager (assuming it's configured correctly)
#     pm = PromptManager(prompt_directory_paths=["path/to/prompts"])
#     llm_provider = LLMProvider(prompt_manager=pm)
#
#     if not llm_provider._api_key:
#          print("API Key not configured. Exiting.")
#          return
#
#     try:
#         # Assuming a prompt named 'example_prompt' version 'v1' exists
#         # And it expects a 'name' in its render data
#         response = await llm_provider.generate_text_async_with_prompt_manager(
#             prompt_name="example_prompt",
#             prompt_version="v1",
#             prompt_render_data={"name": "World"}
#         )
#         print("LLM Response:", response)
#
#         # Example with JSON output (assuming prompt 'json_example' and schema 'MySchema')
#         # class MySchema(BaseModel):
#         #     key: str
#         #     value: int
#         # json_response = await llm_provider.generate_text_async_with_prompt_manager(
#         #     prompt_name="json_example",
#         #     prompt_version="v1",
#         #     prompt_render_data={},
#         #     expected_json_schema=MySchema
#         # )
#         # print("LLM JSON Response:", json_response)
#
#     except Exception as e:
#         print(f"An error occurred: {e}")
#     finally:
#         await llm_provider.close_client()

# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main()) 