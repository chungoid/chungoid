from __future__ import annotations

from typing import Optional, Dict, Any
from pydantic import BaseModel, Field

class TestGeneratorAgentInput(BaseModel):
    code_to_test: str = Field(description="The actual source code string that needs to be tested.")
    file_path_of_code: str = Field(description="The relative path of the file containing the code_to_test. Used for context by the LLM.")
    target_test_file_path: str = Field(description="The intended relative path where the generated test file should eventually be written.")
    test_framework_preference: Optional[str] = Field("pytest", description="Preferred testing framework, e.g., 'pytest', 'unittest'.")
    related_files_context: Optional[Dict[str, str]] = Field(None, description="Content of other relevant files as context for the LLM, e.g., {'models.py': '<content of models.py>'}.")
    programming_language: str = Field("python", description="The programming language of the code to be tested and the tests to be generated.")
    project_root_path: Optional[str] = Field(None, description="Absolute path to the project root, for LLM context if needed.")

class TestGeneratorAgentOutput(BaseModel):
    generated_test_code_string: Optional[str] = Field(None, description="The test code string generated by the LLM.")
    target_test_file_path: str = Field(description="The intended relative path where the generated tests should be written (mirrors input target_test_file_path).")
    status: str = Field(description="Status of the test generation attempt, e.g., 'SUCCESS', 'FAILURE_LLM_GENERATION'.")
    error_message: Optional[str] = Field(None, description="Error message if generation failed.")
    llm_full_response: Optional[str] = Field(None, description="The full raw response from the LLM for debugging.")
    llm_confidence: Optional[float] = Field(None, description="LLM's confidence in the generation, if available.")
    usage_metadata: Optional[Dict[str, Any]] = Field(None, description="LLM usage metadata, e.g., token counts.") 