from __future__ import annotations

from typing import Optional, Dict, Any
from pydantic import BaseModel, Field

class CodeGeneratorAgentInput(BaseModel):
    task_description: str = Field(description="Detailed description of the code to be generated or modified. e.g., 'Implement a FastAPI endpoint /foo that returns {\"bar\": \"baz\"}'")
    target_file_path: str = Field(description="The intended relative path of the file to be created or modified within the project.")
    code_to_modify: Optional[str] = Field(None, description="The existing relevant code snippet if the task is a modification.")
    related_files_context: Optional[Dict[str, str]] = Field(None, description="Content of other relevant files as context for the LLM, e.g., {'utils.py': '<content of utils.py>'}.")
    programming_language: str = Field("python", description="The programming language of the code to be generated.")
    project_root_path: Optional[str] = Field(None, description="Absolute path to the project root, if available and needed for context by the agent/LLM (though agent should primarily use relative paths for output).")

class CodeGeneratorAgentOutput(BaseModel):
    generated_code_string: Optional[str] = Field(None, description="The code string generated by the LLM.")
    target_file_path: str = Field(description="The intended relative path where the generated code should be written (same as input target_file_path).")
    status: str = Field(description="Status of the code generation attempt, e.g., 'SUCCESS', 'FAILURE_LLM_GENERATION', 'FAILURE_INPUT_VALIDATION'.")
    error_message: Optional[str] = Field(None, description="Error message if generation failed.")
    llm_full_response: Optional[str] = Field(None, description="The full raw response from the LLM, if available, for debugging.")
    llm_confidence: Optional[float] = Field(None, description="LLM's confidence in the generation, if available.")
    usage_metadata: Optional[Dict[str, Any]] = Field(None, description="LLM usage metadata, e.g., token counts.") 