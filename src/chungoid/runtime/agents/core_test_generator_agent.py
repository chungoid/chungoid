from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from chungoid.schemas.agent_test_generator import TestGeneratorAgentInput, TestGeneratorAgentOutput
from chungoid.utils.agent_registry_meta import AgentCategory, AgentVisibility
from chungoid.utils.agent_registry import AgentCard

# Placeholder for a real LLM client and prompt templates
# from chungoid.utils.llm_clients import get_llm_client, LLMInterface
# from chungoid.prompts.test_generation import TEST_GENERATION_SYSTEM_PROMPT, TEST_GENERATION_USER_PROMPT_TEMPLATE

logger = logging.getLogger(__name__)

# --- Mock LLM Client (to be replaced) ---
class MockTestLLMClient:
    async def generate_tests(self, system_prompt: str, user_prompt: str, code_to_test: str) -> Dict[str, Any]:
        logger.warning("MockTestLLMClient.generate_tests called. Returning placeholder test code.")
        
        # Simulate generating tests based on the user_prompt and code_to_test
        test_code = f"""# Placeholder tests generated by MockTestLLMClient
# For code in: {user_prompt.split("File Path of Code:")[-1].splitlines()[0].strip()}
# Test framework: {user_prompt.split("Test Framework Preference:")[-1].splitlines()[0].strip()}

import pytest

# Code under test (summary from prompt):
# {code_to_test[:150]}...

def test_placeholder_functionality():
    print("Running placeholder test generated by MockTestLLMClient")
    assert True

"""
        if "class User" in code_to_test:
            test_code += "def test_user_creation():\n    assert True # TODO: Implement real test for User class\n"
        
        import asyncio
        await asyncio.sleep(0.05) # Simulate LLM call latency
        return {
            "generated_test_code": test_code,
            "confidence": 0.80,
            "raw_response": "LLM Raw Output: " + test_code,
            "usage": {"prompt_tokens": 120, "completion_tokens": 60, "total_tokens": 180}
        }
# --- End Mock LLM Client ---

# --- Placeholder Prompts (to be externalized and refined) ---
TEST_GENERATION_SYSTEM_PROMPT = """You are an expert Test Generation AI. 
Given source code, its file path, preferred test framework, and context from related files, generate comprehensive and effective tests. 
Ensure tests cover common cases, edge cases, and error conditions. 
Output only the raw test code string in the specified language and framework, without any surrounding explanations or markdown fences.
"""

TEST_GENERATION_USER_PROMPT_TEMPLATE = """Source Code to Test (from file: {file_path_of_code}):
```{programming_language}
{code_to_test}
```

Target Test File Path: {target_test_file_path}
Programming Language: {programming_language}
Test Framework Preference: {test_framework_preference}

Context from Related Files (if any):
{related_files_formatted_str}

Please generate the test code based on the above information.
"""
# --- End Placeholder Prompts ---

class TestGeneratorAgent:
    AGENT_ID = "CoreTestGeneratorAgent_v1"
    AGENT_NAME = "Core Test Generator Agent"
    VERSION = "0.1.0"
    DESCRIPTION = "Generates test code for given source code, using a specified test framework and optional context from related files."
    CATEGORY = AgentCategory.TEST_GENERATION
    VISIBILITY = AgentVisibility.PUBLIC

    def __init__(self):
        # self.llm_client = get_llm_client(config_for_test_gen_model) # Replace with actual
        self.llm_client = MockTestLLMClient()
        self.system_prompt = TEST_GENERATION_SYSTEM_PROMPT
        self.user_prompt_template = TEST_GENERATION_USER_PROMPT_TEMPLATE

    async def invoke_async(
        self,
        inputs: TestGeneratorAgentInput,
        full_context: Optional[Dict[str, Any]] = None, # Not used by this agent directly yet
    ) -> TestGeneratorAgentOutput:
        logger.info(f"TestGeneratorAgent invoked for code in: {inputs.file_path_of_code}, target test file: {inputs.target_test_file_path}")
        logger.debug(f"TestGeneratorAgent inputs: {inputs}")

        related_files_str = "No related files provided."
        if inputs.related_files_context:
            formatted_ctx_list = []
            for path, content in inputs.related_files_context.items():
                formatted_ctx_list.append(f"--- File: {path} ---\n{content}\n--- End File: {path} ---")
            related_files_str = "\n\n".join(formatted_ctx_list)

        user_prompt = self.user_prompt_template.format(
            code_to_test=inputs.code_to_test,
            file_path_of_code=inputs.file_path_of_code,
            target_test_file_path=inputs.target_test_file_path,
            programming_language=inputs.programming_language,
            test_framework_preference=inputs.test_framework_preference or "pytest",
            related_files_formatted_str=related_files_str
        )

        try:
            llm_response_dict = await self.llm_client.generate_tests(
                system_prompt=self.system_prompt,
                user_prompt=user_prompt,
                code_to_test=inputs.code_to_test
            )

            generated_tests = llm_response_dict.get("generated_test_code")
            if not generated_tests or not isinstance(generated_tests, str):
                logger.error("LLM did not return a valid test code string.")
                return TestGeneratorAgentOutput(
                    target_test_file_path=inputs.target_test_file_path,
                    status="FAILURE_LLM_GENERATION",
                    error_message="LLM did not return a valid test code string in its response.",
                    llm_full_response=str(llm_response_dict)
                )

            return TestGeneratorAgentOutput(
                generated_test_code_string=generated_tests,
                target_test_file_path=inputs.target_test_file_path,
                status="SUCCESS",
                llm_full_response=llm_response_dict.get("raw_response"),
                llm_confidence=llm_response_dict.get("confidence"),
                usage_metadata=llm_response_dict.get("usage")
            )

        except Exception as e:
            logger.exception(f"Error during TestGeneratorAgent LLM call or processing: {e}")
            return TestGeneratorAgentOutput(
                target_test_file_path=inputs.target_test_file_path,
                status="FAILURE_LLM_GENERATION",
                error_message=f"LLM interaction failed: {str(e)}"
            )

    @staticmethod
    def get_agent_card_static() -> AgentCard:
        """Returns the static AgentCard for the TestGeneratorAgent."""
        return AgentCard(
            agent_id=TestGeneratorAgent.AGENT_ID,
            name=TestGeneratorAgent.AGENT_NAME,
            version=TestGeneratorAgent.VERSION,
            description=TestGeneratorAgent.DESCRIPTION,
            category=TestGeneratorAgent.CATEGORY,
            visibility=TestGeneratorAgent.VISIBILITY,
            input_schema=TestGeneratorAgentInput.model_json_schema(),
            output_schema=TestGeneratorAgentOutput.model_json_schema(),
        )

# Basic test stub
async def main_test_test_gen():
    logging.basicConfig(level=logging.DEBUG)
    agent = TestGeneratorAgent()

    sample_code = "def add(a, b):\n    return a + b\n\ndef subtract(a,b):\n    return a-b"

    test_input = TestGeneratorAgentInput(
        code_to_test=sample_code,
        file_path_of_code="math_lib.py",
        target_test_file_path="test_math_lib.py",
        test_framework_preference="pytest",
        programming_language="python"
    )
    output = await agent.invoke_async(test_input)
    print("--- Test Generation Output ---")
    print(f"Status: {output.status}")
    if output.error_message:
        print(f"Error: {output.error_message}")
    if output.generated_test_code_string:
        print(f"Generated Tests for {output.target_test_file_path}:\n{output.generated_test_code_string}")
    print(f"Confidence: {output.llm_confidence}")
    print("----------------------------\n")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main_test_test_gen()) 