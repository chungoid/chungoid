from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from chungoid.schemas.agent_code_generator import CodeGeneratorAgentInput, CodeGeneratorAgentOutput
from chungoid.utils.agent_registry_meta import AgentCategory, AgentVisibility
from chungoid.utils.agent_registry import AgentCard

# Placeholder for a real LLM client and prompt templates
# from chungoid.utils.llm_clients import get_llm_client, LLMInterface
# from chungoid.prompts.code_generation import CODE_GENERATION_SYSTEM_PROMPT, CODE_GENERATION_USER_PROMPT_TEMPLATE

logger = logging.getLogger(__name__)

# --- Mock LLM Client (to be replaced) ---
class MockCodeLLMClient:
    async def generate_code(self, system_prompt: str, user_prompt: str, code_context: Optional[str] = None) -> Dict[str, Any]:
        logger.warning("MockCodeLLMClient.generate_code called. Returning placeholder code.")
        
        # Simulate generating code based on the user_prompt (which contains task_description)
        generated_code = f"# Placeholder code generated by MockCodeLLMClient\n# Task: {user_prompt[:100]}...\nprint('Hello from generated code!')\n"
        if "class Foo" in user_prompt:
            generated_code += "class Foo:\n    pass\n"
        elif "def bar" in user_prompt:
            generated_code += "def bar():\n    return \"bar_was_called\"\n"
        
        import asyncio
        await asyncio.sleep(0.05) # Simulate LLM call latency
        return {
            "generated_code": generated_code,
            "confidence": 0.85,
            "raw_response": "LLM Raw Output: " + generated_code,
            "usage": {"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150}
        }
# --- End Mock LLM Client ---

# --- Placeholder Prompts (to be externalized and refined) ---
CODE_GENERATION_SYSTEM_PROMPT = """You are an expert Code Generation AI. 
Given a task description, existing code (if any), and related file contexts, generate the required code in the specified language. 
Ensure the code is clean, efficient, and well-commented where necessary. 
Output only the raw code string requested, without any surrounding explanations or markdown fences unless the task specifically asks for that format.
"""

CODE_GENERATION_USER_PROMPT_TEMPLATE = """Task Description: {task_description}
Programming Language: {programming_language}
Target File Path: {target_file_path}

Existing Code Snippet (if modifying):
```
{code_to_modify}
```

Context from Related Files:
{related_files_formatted_str}

Please generate the code based on the above information.
"""
# --- End Placeholder Prompts ---

class CodeGeneratorAgent:
    AGENT_ID = "CoreCodeGeneratorAgent_v1"
    AGENT_NAME = "Core Code Generator Agent"
    VERSION = "0.1.0"
    DESCRIPTION = "Generates source code based on a task description, optionally modifying existing code and using context from related files."
    CATEGORY = AgentCategory.CODE_GENERATION
    VISIBILITY = AgentVisibility.PUBLIC

    def __init__(self):
        # self.llm_client = get_llm_client(config_for_code_gen_model) # Replace with actual
        self.llm_client = MockCodeLLMClient()
        self.system_prompt = CODE_GENERATION_SYSTEM_PROMPT
        self.user_prompt_template = CODE_GENERATION_USER_PROMPT_TEMPLATE

    async def invoke_async(
        self,
        inputs: CodeGeneratorAgentInput,
        full_context: Optional[Dict[str, Any]] = None, # Not used by this agent directly yet
    ) -> CodeGeneratorAgentOutput:
        logger.info(f"CodeGeneratorAgent invoked for target: {inputs.target_file_path}")
        logger.debug(f"CodeGeneratorAgent inputs: {inputs}")

        related_files_str = "No related files provided."
        if inputs.related_files_context:
            formatted_ctx_list = []
            for path, content in inputs.related_files_context.items():
                formatted_ctx_list.append(f"--- File: {path} ---\n{content}\n--- End File: {path} ---")
            related_files_str = "\n\n".join(formatted_ctx_list)

        user_prompt = self.user_prompt_template.format(
            task_description=inputs.task_description,
            programming_language=inputs.programming_language,
            target_file_path=inputs.target_file_path,
            code_to_modify=inputs.code_to_modify or "# No existing code provided for modification.",
            related_files_formatted_str=related_files_str
        )

        code_context_for_llm = inputs.code_to_modify # Or a more structured context

        try:
            llm_response_dict = await self.llm_client.generate_code(
                system_prompt=self.system_prompt,
                user_prompt=user_prompt,
                code_context=code_context_for_llm
            )

            generated_code = llm_response_dict.get("generated_code")
            if not generated_code or not isinstance(generated_code, str):
                logger.error("LLM did not return a valid code string.")
                return CodeGeneratorAgentOutput(
                    target_file_path=inputs.target_file_path,
                    status="FAILURE_LLM_GENERATION",
                    error_message="LLM did not return a valid code string in its response.",
                    llm_full_response=str(llm_response_dict)
                )

            return CodeGeneratorAgentOutput(
                generated_code_string=generated_code,
                target_file_path=inputs.target_file_path,
                status="SUCCESS",
                llm_full_response=llm_response_dict.get("raw_response"),
                llm_confidence=llm_response_dict.get("confidence"),
                usage_metadata=llm_response_dict.get("usage")
            )

        except Exception as e:
            logger.exception(f"Error during CodeGeneratorAgent LLM call or processing: {e}")
            return CodeGeneratorAgentOutput(
                target_file_path=inputs.target_file_path,
                status="FAILURE_LLM_GENERATION",
                error_message=f"LLM interaction failed: {str(e)}"
            )

    @staticmethod
    def get_agent_card_static() -> AgentCard:
        """Returns the static AgentCard for the CodeGeneratorAgent."""
        return AgentCard(
            agent_id=CodeGeneratorAgent.AGENT_ID,
            name=CodeGeneratorAgent.AGENT_NAME,
            version=CodeGeneratorAgent.VERSION,
            description=CodeGeneratorAgent.DESCRIPTION,
            category=CodeGeneratorAgent.CATEGORY,
            visibility=CodeGeneratorAgent.VISIBILITY,
            input_schema=CodeGeneratorAgentInput.model_json_schema(),
            output_schema=CodeGeneratorAgentOutput.model_json_schema(),
        )

# Alias the static method for module-level import
get_agent_card_static = CodeGeneratorAgent.get_agent_card_static

# Basic test stub
async def main_test_code_gen():
    logging.basicConfig(level=logging.DEBUG)
    agent = CodeGeneratorAgent()

    test_input_create = CodeGeneratorAgentInput(
        task_description="Create a simple Python function `add(a, b)` that returns their sum.",
        target_file_path="math_utils.py",
        programming_language="python"
    )
    output_create = await agent.invoke_async(test_input_create)
    print("--- Create Function Output ---")
    print(f"Status: {output_create.status}")
    if output_create.error_message:
        print(f"Error: {output_create.error_message}")
    if output_create.generated_code_string:
        print(f"Generated Code for {output_create.target_file_path}:\n{output_create.generated_code_string}")
    print(f"Confidence: {output_create.llm_confidence}")
    print("----------------------------\n")

    test_input_modify = CodeGeneratorAgentInput(
        task_description="Add a docstring to the existing function `subtract(a,b)`.",
        target_file_path="math_utils.py",
        code_to_modify="def subtract(a, b):\n    return a - b",
        programming_language="python",
        related_files_context={"helper.py": "MAX_VALUE = 100"}
    )
    output_modify = await agent.invoke_async(test_input_modify)
    print("--- Modify Function Output ---")
    print(f"Status: {output_modify.status}")
    if output_modify.error_message:
        print(f"Error: {output_modify.error_message}")
    if output_modify.generated_code_string:
        print(f"Generated Code for {output_modify.target_file_path}:\n{output_modify.generated_code_string}")
    print(f"Confidence: {output_modify.llm_confidence}")
    print("----------------------------\n")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main_test_code_gen()) 