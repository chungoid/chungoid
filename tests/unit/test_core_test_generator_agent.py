import pytest
from unittest.mock import patch, AsyncMock

from chungoid.runtime.agents.core_test_generator_agent import TestGeneratorAgent, MockTestLLMClient
from chungoid.schemas.agent_test_generator import TestGeneratorAgentInput, TestGeneratorAgentOutput

@pytest.mark.asyncio
async def test_test_generator_agent_create_tests_success():
    """Test successful test generation for given code."""
    agent = TestGeneratorAgent()
    code_str = "def my_function(x):\n    return x * 2"
    file_path = "source_module.py"
    test_file_path = "test_source_module.py"
    
    inputs = TestGeneratorAgentInput(
        code_to_test=code_str,
        file_path_of_code=file_path,
        target_test_file_path=test_file_path,
        programming_language="python",
        test_framework_preference="pytest"
    )

    # MockLLMClient's default behavior will be used.
    output = await agent.invoke_async(inputs)

    assert output.status == "SUCCESS"
    assert output.error_message is None
    assert output.target_test_file_path == test_file_path
    assert output.generated_test_code_string is not None
    assert "# Placeholder tests generated by MockTestLLMClient" in output.generated_test_code_string
    assert f"# For code in: {file_path}" in output.generated_test_code_string 
    assert "# Test framework: pytest" in output.generated_test_code_string
    assert output.llm_confidence is not None
    assert output.usage_metadata is not None

@pytest.mark.asyncio
async def test_test_generator_agent_with_related_context():
    """Test test generation when related_files_context is provided."""
    agent = TestGeneratorAgent()
    code_str = "from .models import Item\ndef process_item(item: Item):\n    return item.name"
    file_path = "services.py"
    test_file_path = "test_services.py"
    related_ctx = {"models.py": "class Item:\n    name: str"}

    inputs = TestGeneratorAgentInput(
        code_to_test=code_str,
        file_path_of_code=file_path,
        target_test_file_path=test_file_path,
        related_files_context=related_ctx,
        programming_language="python"
    )

    # Spy on the LLM call to check prompt components
    captured_prompts = {}
    async def generate_tests_spy(system_prompt: str, user_prompt: str, code_to_test: str):
        captured_prompts['system'] = system_prompt
        captured_prompts['user'] = user_prompt
        captured_prompts['code_to_test_arg'] = code_to_test # Actual code passed to LLM method
        # Return default mock LLM output
        mock_llm = MockTestLLMClient()
        return await mock_llm.generate_tests(system_prompt, user_prompt, code_to_test)

    with patch.object(MockTestLLMClient, 'generate_tests', side_effect=generate_tests_spy) as mock_llm_call:
        output = await agent.invoke_async(inputs)
    
    assert output.status == "SUCCESS"
    assert output.generated_test_code_string is not None
    mock_llm_call.assert_called_once()

    # Check that inputs were part of the prompt passed to LLM
    assert code_str in captured_prompts['user']
    assert file_path in captured_prompts['user']
    assert "models.py" in captured_prompts['user']
    assert related_ctx["models.py"] in captured_prompts['user']
    assert code_str == captured_prompts['code_to_test_arg']

@pytest.mark.asyncio
async def test_test_generator_agent_llm_returns_invalid_response():
    """Test failure when LLM response does not contain a valid test code string."""
    agent = TestGeneratorAgent()
    inputs = TestGeneratorAgentInput(code_to_test="def f(): pass", file_path_of_code="f.py", target_test_file_path="test_f.py")

    with patch.object(MockTestLLMClient, 'generate_tests', new_callable=AsyncMock) as mock_llm_call:
        mock_llm_call.return_value = {"confidence": 0.3, "raw_response": "No code here"}
        
        output = await agent.invoke_async(inputs)

    assert output.status == "FAILURE_LLM_GENERATION"
    assert output.error_message == "LLM did not return a valid test code string in its response."
    assert output.generated_test_code_string is None
    assert str(mock_llm_call.return_value) in output.llm_full_response

@pytest.mark.asyncio
async def test_test_generator_agent_llm_call_exception():
    """Test failure when the LLM call itself raises an exception."""
    agent = TestGeneratorAgent()
    inputs = TestGeneratorAgentInput(code_to_test="def f(): pass", file_path_of_code="f.py", target_test_file_path="test_f.py")

    with patch.object(MockTestLLMClient, 'generate_tests', new_callable=AsyncMock) as mock_llm_call:
        mock_llm_call.side_effect = Exception("LLM service down")
        
        output = await agent.invoke_async(inputs)

    assert output.status == "FAILURE_LLM_GENERATION"
    assert "LLM interaction failed: LLM service down" in output.error_message
    assert output.generated_test_code_string is None